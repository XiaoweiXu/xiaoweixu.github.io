<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8"/>
        <title>[ Xiaowei Xu | News ]</title>
        <meta name="viewport" content="width=device-width, initial-scale=1.0"/>

        <!-- Font Awesome Icons -->
        <link rel="stylesheet" href="../css/font-awesome.min.css"/>

        <!-- Bootstrap -->
        <link href="../css/bootstrap.min.css" rel="stylesheet"/>
        <!--<link href="css/bootstrap.min.css" rel="stylesheet">-->

        <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
        <!--[if lt IE 9]>
         <script src="js/html5shiv.js"></script>
         <script src="js/respond.min.js"></script>
         <![endif]-->


        <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
        <script src="../js/jquery.min.js"></script>
        <!-- Include all compiled plugins (below), or include individual files as needed -->
        <script src="../js/bootstrap.min.js"></script>
        <script src="../js/menucollapse.js"></script>
        <script type="text/javascript" src="js/arrow78.js"></script>
        <script type="text/javascript" src="js/custom.js"></script>

        <body id="page-top" class="index">

            <!-- Navigation -->
            <nav class="navbar navbar-default navbar-fixed-top">
                <div class="container-fluid">
                    <!-- Brand and toggle get grouped for better mobile display -->
                    <div class="navbar-header">
                        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"  data-target="#bs-example-navbar-collapse-2" aria-expanded="false">
                            <span class="sr-only">Toggle navigation</span>
                            <span class="glyphicon glyphicon-search"></span>
                        </button>
                        <button id="button2" type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
                            <span class="sr-only">Toggle navigation</span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                        </button>
                        <span><a href="http://www.gdghospital.org.cn/ziwang/index.php/"><img border="0" width="200" src="../images/gdsxxgbyjs_xxw.png"/></a></span>
                        <span><a href="http://english.hust.edu.cn/"><img border="0" width="40" src="../images/Hustseals.png"/></a></span>
                        <span><a href="https://www.ualberta.ca/index.html"><img border="0" width="40" src="../images/ua.png"/></a></span>
                        <span><a href="http://www.buffalo.edu/"><img border="0" width="40" src="../images/SUNY-Buffalo.png"/></a></span>
                        <span><a href="https://www.zju.edu.cn/english/"><img border="0" width="40" src="../images/zju.png"/></a></span>
                        <span><a href="https://www.nd.edu/"><img border="0" width="40" src="../images/nd3.png"/></a></span>
                    </div>

                    <!-- Collect the nav links, forms, and other content for toggling -->
                    <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                        <ul class="nav navbar-nav navbar-right">
                            <li class="dropdown">
                                <a class="dropdown-toggle" data-toggle="dropdown" href="#">
                                    Home<span class="caret"></span></a>
                                <ul class="dropdown-menu">
                                    <li><a href="../index.html#intro">Introduction</a></li>
                                    <!-- <li><a href="bio.html">Biography</a></li> -->
                                    <li><a href="../cv.html">CV (Web)</a></li>
                                    <li><a target="_blank" href="../cv_xiaoweixu.pdf">CV (PDF)</a></li>
                                </ul>
                            </li>
                            <li class="page-scroll">
                                <a href="../index.html#news">News</a>
                            </li>
                            <li class="dropdown">
                                <a class="dropdown-toggle" data-toggle="dropdown" href="#">
                                    Publications<span class="caret"></span></a>
                                <ul class="dropdown-menu">
                                    <li><a onclick="javascript:reset_menus();$('#tab-3-content').show();" href="../index.html#publications">Selected</a></li>
                                    <li><a href="../publications/">All</a></li>
                                </ul>
                            </li>
                            <!-- <li class="page-scroll">
                                <a href="talks/">Talks</a>
                            </li>
                            <li class="page-scroll">
                                <a href="index.html#courses">Courses</a>
                            </li>
                            <li class="page-scroll">
                                <a href="index.html#awards">Awards</a>
                            </li> -->
                            <li class="page-scroll">
                                <a href="../index.html#service">Service</a>
                            </li>
                            <li class="page-scroll">
                                <a href="../index.html#resources">Resources</a>
                            </li>
                            <!-- <li class="page-scroll">
                                <a target="_blank" href="http://dmsl.cs.ucy.ac.cy/projects.php">Grants</a>
                            </li> -->

                            <li class="page-scroll">
                                <a href="../index.html#contact">Contact</a>
                            </li>
                            <li class="page-scroll">
                                <a onclick="$('#bs-example-navbar-collapse-2').toggle();">
                                    <span class="glyphicon glyphicon-search"></span>
                                </a>
                            </li>
                        </ul>
                    </div><!-- /.navbar-collapse -->

                    <!-- search box submenu -->
                    <div class="collapse" id="bs-example-navbar-collapse-2">
                        <gcse:search></gcse:search>
                    </div>

                </div><!-- /.container-fluid -->
            </nav>

            <section>
                <!-- Place this tag where you want the search results to render -->
                <gcse:searchresults-only></gcse:searchresults-only>
            </section>

            <section id="tree" style="margin-top:50px">
                <div class="container">
                    <a href="../index.html">Xiaowei Xu</a> > Recordings
                </div>
            </section>


            <!-- Home Section -->
            <section id="home">
                <div class="container lead">

                    <h3> Oct, 2020 </h3>
                    
                    <li> <span class="label label-success">23 Oct, 2020</span> <a href="https://arxiv.org/pdf/1903.05631.pdf">"ST-UNet: A Spatio-Temporal U-Network for
                        Graph-structured Time Series Modeling" </a> </li>
                        <p style=text-align:justify> <span class="label label-default">extensive</span> </p> This work is for spatio-temporal graph data, and this U-shaped network used a paired sampling operation in spacetime domain
                        accordingly.</p>

                        <p><span class="label label-default">comment</span> </p> This can be used for segmentation.</p>
                    <li><span class="label label-success">23 Oct, 2020</span> <a href="https://openaccess.thecvf.com/content_CVPRW_2019/papers/PBVS/Bhatt_Segmentation_of_Low-Level_Temporal_Plume_Patterns_From_IR_Video_CVPRW_2019_paper.pdf">"Segmentation of Low-Level Temporal Plume Patterns from IR Vide" </a> </li>
                        <p style=text-align:justify> <span class="label label-default">extensive</span> </p> This work ONLY used 2D conv and 3D conv to extract teporal and spatial features, respectively. And it can be further combined with LSTM-Conv with boosted performance.
                        
                        <!-- [comment] This can be on. -->
                    <li><span class="label label-success">24-26 Oct, 2020</span> <a href="https://arxiv.org/pdf/2001.05566.pdf">"Image Segmentation Using Deep Learning: A Survey" </a> </li>
                        <p style=text-align:justify> <span class="label label-default">Useful professional items</span> </p> elementwise; receptive field;  pixel-wise predictions; Skip connections combine coarse, high-level defaultrmation and fine,
                        low-level information;

                        <p style=text-align:justify> <span class="label label-default">Fully Convolutional Networks</span> </p> first work; not fast enough for real-time inference, it does not take
                        into account the global context information in an efficient way, and it is not easily transferable to 3D images

                        <p> <span class="label label-default">Convolutional Models With Graphical Models</span> </p> Integrate more context; </p>
                        <p style=text-align:justify>Incorporate probabilistic graphical models, such as Conditional Random Fields (CRFs) and Markov Random Field (MRFs), into DL architectures; </p>
                            poor localization property of deep CNNs (responses from the final layer of deep CNNs are not sufficiently localized for accurate
                            object segmentation (due to the invariance properties that make CNNs good for high level tasks such as classification))；</p>

                            <p> <span class="label label-default">Encoder-Decoder Based Models</span> </p>
                            <p style=text-align:justify>SegNet: The main novelty of SegNet is in the way the decoder upsamples its lower resolution input feature map. specifically, it
                            uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. 
                            This eliminates the need for learning to up-sample. SegNet is also significantly smaller in the number of trainable parameters
                            than other competing architectures.</p>
                            <p style=text-align:justify>HRNet: maintains high-resolution representations through the encoding process by connecting the
                            high-to-low resolution convolution streams in parallel, and
                            repeatedly exchanging the information across resolutions</p>
                            <p style=text-align:justify>U-net: Feature maps from the down-sampling part of the network are copied to the up-sampling part to avoid losing pattern
                            information.</p>
                            <p style=text-align:justify>V-Net:  introduced a new objective function based on the Dice coefficient, enabling the model to deal with situations in
                            which there is a strong imbalance between the number of voxels in the foreground and background.</p>

                        <p> <span class="label label-default">Multi-Scale and Pyramid Network Based Models</span> </p>
                        <p style=text-align:justify>Multi-scale analysis, a rather old idea in image processing,
                        has been deployed in various neural network architectures.
                        One of the most prominent models of this sort is the Feature
                        Pyramid Network (FPN) proposed by Lin.</p>
                        <p style=text-align:justify>The apparent spatial resolution of convolutional feature maps is low, the high-dimensional feature representation contains significant sub-pixel localization
                        information.</p>

                        <p> <span class="label label-default">R-CNN Based Models (for Instance Segmentation)</span> </p>
                        <p style=text-align:justify>Mask R-CNN: essentially a Faster RCNN with 3 output branches (Figure 19)—the first computes
                        the bounding box coordinates, the second computes the
                        associated classes, and the third computes the binary mask to segment the object.  </p>
                        <p style=text-align:justify>It is worth noting that there is
                        another promising research direction that attempts to solve
                        the instance segmentation problem by learning grouping
                        cues for bottom-up segmentation. </p>

                        <p> <span class="label label-default">Dilated Convolutional Models and DeepLab Family</span> </p>
                        <p style=text-align:justify>Dilated convolution: to address the decreasing resolution in the network (caused by max-pooling and striding).</p>
                        Atrous Spatial Pyramid Pooling (ASPP): probes an incoming convolutional feature layer with filters at multiple
                        sampling rates, thus capturing objects as well as image context at multiple scales to robustly segment objects at
                        multiple scales.</p>
                        <p style=text-align:justify>Improved localization of object boundaries by combining methods from deep CNNs and probabilistic graphical models.</p>

                        <p> <span class="label label-default">Recurrent Neural Network Based Models</span> </p>
                        <p style=text-align:justify>RNNs are useful in modeling the short/long term dependencies among pixels to
                        (potentially) improve the estimation of the segmentation map.
                        Using RNNs, pixels may be linked together and processed
                        sequentially to model global contexts and improve semantic
                        segmentation. One challenge, though, is the natural 2D
                        structure of images.</p>
                        <p style=text-align:justify>Liang et al.: Instead of evenly dividing an image to pixels or patches
                        in existing multi-dimensional LSTM structures (e.g., row,
                        grid and diagonal LSTMs), they take each arbitrary-shaped
                        superpixel as a semantically consistent node, and adaptively
                        construct an undirected graph for the image, where the spatial relations of the superpixels are naturally used as edges.

                        <p> <span class="label label-default">Attention-Based Models</span> </p>
                        <p style=text-align:justify>Attention mechanisms have been persistently explored in
                        computer vision over the years, and it is therefore not
                        surprising to find publications that apply such mechanisms
                        to semantic segmentation.

                        <p> <span class="label label-default">Generative Models and Adversarial Training</span> </p>
                        <p style=text-align:justify>Since their introduction, GANs have been applied to a wide
                        range tasks in computer vision, and have been adopted for
                        image segmentation too.

                        <p> <span class="label label-default">CNN Models With Active Contour Models</span> </p>
                        <p style=text-align:justify>The exploration of synergies between FCNs and Active
                        Contour Models (ACMs) [7] has recently attracted research
                        interest.

                        <span><img border="0" width="1200" src="seg_models.png"/></a></span>

                        <p> <span class="label label-default">Callenges</span> 
                        <p style=text-align:justify><p style=text-align:justify>More Challenging Datasets; Interpretable Deep Models; Weakly-Supervised and Unsupervised Learning; Real-time Models for Various Applications;
                        Memory Efficient Models; 

                        
                        <p> <span class="label label-default">comment</span> 
                        <p style=text-align:justify>It is worth mentioning that some of these works, use data
                        augmentation to increase the number of labeled samples,
                        specially the ones which deal with small datasets (such
                        as in medical domain).</p>
                        <p style=text-align:justify>Some typical transformations
                        include translation, reflection, rotation, warping, scaling,
                        color space shifting, cropping, and projections onto principal
                        components.</p>
                        <p style=text-align:justify>For some small datasets, data augmentation has been shown
                        to boost model performance more than 20%.</p>
                        Collecting labeled samples for segmentation
                        problem is problematic in many application domains, particularly so in medical image analysis. </p>

                        

                    <li><span class="label label-success">27 Oct, 2020</span><a href="https://arxiv.org/pdf/1804.02967.pdf">"HyperDense-Net: A hyper-densely connected
                        CNN for multi-modal image segmentation" </a> </li>
                        <p style=text-align:justify> <span class="label label-default">extensive</span>  </p>
                            Dense connections have attracted substantial attention in computer vision because they facilitate gradient flow
                            and implicit deep supervision during training.</p>
                            <p style=text-align:justify><b>Why DenseNet is powerful?</b> DenseNets are built on the idea that adding direct
                            connections from any layer to all the subsequent layers
                            in a feed-forward manner makes training easier and more
                            accurate. This is motivated by three observations. First,
                            there is an implicit deep supervision thanks to the short
                            paths to all feature maps in the architecture. Second, direct
                            connections between all layers help improving the flow of
                            information and gradients throughout the entire network.
                            Third, dense connections have a regularizing effect, which
                            reduces the risk of over-fitting on tasks with smaller training
                            sets.</p>
                            <p style=text-align:justify>Advances in multi-modal imaging, however, come at the
                            price of an inherently large amount of data, imposing a
                            burden on disease assessments. Visual inspections of such
                            an enormous amount of medical images are prohibitively
                            time-consuming, prone to errors and unsuitable for largescale studies.</p>
                            Fusion of multi-modal CNN feature representations: early-fusion methods (simply merged at the input of the network,  assume
                            that the relationship between different modalities are simple(e.g., linear), 
                            difficult to discover highly non-linear relationships between the low-level features of different modalities), 
                            late fusion methods (independent convolutional network for each modality, and
                            fused the outputs of the different networks in higher-level layers).</p>

                            <p style=text-align:justify><b>Shuffling and interleaving feature map elements</b> in a
                            CNN was recently found to enhance the efficiency and
                            performance, while serving as a strong regularizer [44]–
                            [46]. This is motivated by the fact that intermediate CNN
                            layers perform deterministic transformations to improve the
                            performance, however, relevant information might be lost
                            during these operations [47]. To overcome this issue, it is
                            therefore beneficial for intermediate layers to offer a variety
                            of information exchange while preserving the aforementioned deterministic functions.</p>
                        
                        <p> <span class="label label-default">comment</span> 
                            <p style=text-align:justify>Each imaging modality has a path, and dense connections occur not only
                            between the pairs of layers within the same path, but also
                            between those across different paths.</p>
                            <p style=text-align:justify>This work is a very good and representative work to follow on both ideas, experiments and writting. 
                            Actually the idea of this work is not very creative (relatively easy to get the idea), but the overall experiment and discussion show deep understanding of the area, and solid results.
                            
                            
                    <li><span class="label label-success">28 Oct, 2020</span><a href="https://arxiv.org/pdf/1709.07330.pdf">"H-DenseUNet: Hybrid Densely Connected UNet for
                        Liver and Tumor Segmentation from CT Volumes" </a> </li>
                        <p> <span class="label label-default">extensive/intensive</span>  </p>
                            2D convolutions can not fully
                                leverage the spatial information along the third dimension while
                                3D convolutions suffer from high computational cost and GPU
                                memory consumption.</p>
                            Compared to
                                2D FCNs, 3D FCNs suffer from high computational cost and
                                GPU memory consumption. The high memory consumption
                                limits the depth of the network as well as the filter’s field-of-view, which are the two key factors for performance gains [22].</p>
                            Hybrid densely connected UNet (H-DenseUNet), which
                                consists of a 2D DenseUNet for efficiently extracting intra-slice
                                features and a 3D counterpart for hierarchically aggregating
                                volumetric contexts under the spirit of the auto-context algorithm.            

                        <p> <span class="label label-default">comment</span>  </p>
                        We can show the performance gain using the training loss, which is a good way for presentation.  </p>
                            Indepth analysis of the proposed  method such as the improvement on dataset with different size of tumors is an good example of a solid work.

                            
                    <li><span class="label label-success">29 Oct, 2020</span><a href="https://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks.pdf">"How transferable are features in deep neural networks?" </a> </li>
                        <p> <span class="label label-default">extensive</span> 
                            Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters
                                and color blobs. Such first-layer features appear not to be specific to a particular
                                dataset or task, but general in that they are applicable to many datasets and tasks.</p>
                                The usual transfer learning approach is to train a base network and then copy its first n layers to
                                the first n layers of a target network. The remaining layers of the target network are then randomly
                                initialized and trained toward the target task. One can choose to backpropagate the errors from
                                the new task into the base (copied) features to fine-tune them to the new task, or the transferred
                                feature layers can be left frozen, meaning that they do not change during training on the new task.
                                The choice of whether or not to fine-tune the first n layers of the target network depends on the
                                size of the target dataset and the number of parameters in the first n layers. If the target dataset is
                                small and the number of parameters is large, fine-tuning may result in overfitting, so the features
                                are often left frozen. On the other hand, if the target dataset is large or the number of parameters is
                                small, so that overfitting is not a problem, then the base features can be fine-tuned to the new task
                                to improve performance. Of course, if the target dataset is very large, there would be little need to
                                transfer because the lower level filters could just be learned from scratch on the target dataset.</p>
                                Transferability is negatively affected by two distinct issues: (1) the specialization of
                                higher layer neurons to their original task at the expense of performance on the
                                target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. </p>
                            The original network contained <b>fragile co-adapted features</b> on successive layers, that is, features
                                that interact with each other in a complex or fragile way such that this co-adaptation could not be
                                relearned by the upper layers alone. Gradient descent was able to find a good solution the first
                                time, but this was only possible because the layers were jointly trained.</p>
                            Transferring features will boost generalization performance even if the target dataset is large.</p>

                        
                        <p> <span class="label label-default">comment</span> This paper gives some understandings of transfer learning.

                    <li><span class="label label-success">30 Oct, 2020</span><a href="https://arxiv.org/pdf/2004.03466.pdf">"U-Net Using Stacked Dilated Convolutions for Medical
                        Image Segmentation" </a> </li>
                        <p> <span class="label label-default">extensive</span> </p>
                            Unlike vanilla U-Net which incorporates two standard convolutions
                                in each encoder/decoder operation, SDU-Net uses one standard convolution followed by multiple dilated convolutions and concatenates all dilated convolution
                                outputs as input to the next operation. </p>
                            U-Net convolutions have a very limited receptive field and encoder
                                downsampling may degrade correlation between the pixels. To obtain larger receptive
                                fields, Devalla et al. [12] introduced dilated convolutions into U- Net, where the dilation rate was increased while the resolution was downsampled.</p>
                            However, Hamaguchi et al. [13] pointed out that aggressively increasing dilation rates might fail to aggregate local features due to sparsity of 
                                the kernel and potentially be detrimental to small objects. </p>

                        <p> <span class="label label-default">comment</span> There exists multiple receptive fields within the same resolution which is the reason why it can process both large and small objects.  

                    <li><span class="label label-success">31 Oct, 2020</span><a href="https://www.groundai.com/project/a-joint-3d-unet-graph-neural-network-based-method-for-airway-segmentation-from-chest-cts/1">"A joint 3D UNet-Graph Neural Network-based method for Airway Segmentation from chest CTs" </a> </li>
                        <p> <span class="label label-default">extensive/intensive</span> </p>
                            3D UNet architecture with a graph neural network (GNN) model. In this approach, the convolutional layers at the deepest level of the UNet are replaced by a GNN-based module with a series of graph convolutions. 
                            The dense feature maps at this level are transformed into a graph input to the GNN module.</p>
                            The incorporation of graph convolutions in the UNet provides nodes in the graph with information that is based on node connectivity, in addition to the local features learnt through the downsampled paths. 
                            This information can help improve segmentation decisions.</p>
                            Existing works: 3D UNet cannot capture various small terminal branches. Also, the GNNs have been applied to airway extraction as a graph refinement approach in [8]. However, this method was not end-to-end optimised and relied on handcrafted features as input to the graph.</p>


                        <p> <span class="label label-default">comment</span> The idea is good but the performance improvement is not significant, which needs more investigations..  


                </div>
                </div>
            </section>

            <hr class="star-primary">
            <footer>
                <small>
                    <center>
                        © 2016 | D. Zeinalipour. Credits: AR template
                        <a onclick="javascript:$('#credit').toggle();"><img border="0" src="images/ccby.png"/></a>
                        <div style="display:none;" id="credit">[AR template available under Creative Commons CC BY 4.0 licence:
                            <a href="https://github.com/dmsl/academic-responsive-template" target="_blank">
                                https://github.com/dmsl/academic-responsive-template
                            </a> ]
                        </div>
                    </center>
                </small>
            </footer>
            
        </body>
        
</html>
