<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8"/>
        <title>[ Xiaowei Xu | News ]</title>
        <meta name="viewport" content="width=device-width, initial-scale=1.0"/>

        <!-- Font Awesome Icons -->
        <link rel="stylesheet" href="../css/font-awesome.min.css"/>

        <!-- Bootstrap -->
        <link href="../css/bootstrap.min.css" rel="stylesheet"/>
        <!--<link href="css/bootstrap.min.css" rel="stylesheet">-->

        <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
        <!--[if lt IE 9]>
         <script src="js/html5shiv.js"></script>
         <script src="js/respond.min.js"></script>
         <![endif]-->


        <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
        <script src="../js/jquery.min.js"></script>
        <!-- Include all compiled plugins (below), or include individual files as needed -->
        <script src="../js/bootstrap.min.js"></script>
        <script src="../js/menucollapse.js"></script>
        <script type="text/javascript" src="js/arrow78.js"></script>
        <script type="text/javascript" src="js/custom.js"></script>

        <body id="page-top" class="index">

            <!-- Navigation -->
            <nav class="navbar navbar-default navbar-fixed-top">
                <div class="container-fluid">
                    <!-- Brand and toggle get grouped for better mobile display -->
                    <div class="navbar-header">
                        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"  data-target="#bs-example-navbar-collapse-2" aria-expanded="false">
                            <span class="sr-only">Toggle navigation</span>
                            <span class="glyphicon glyphicon-search"></span>
                        </button>
                        <button id="button2" type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
                            <span class="sr-only">Toggle navigation</span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                        </button>
                        <span><a href="http://www.gdghospital.org.cn/ziwang/index.php/"><img border="0" width="200" src="../images/gdsxxgbyjs_xxw.png"/></a></span>
                        <span><a href="http://english.hust.edu.cn/"><img border="0" width="40" src="../images/Hustseals.png"/></a></span>
                        <span><a href="https://www.ualberta.ca/index.html"><img border="0" width="40" src="../images/ua.png"/></a></span>
                        <span><a href="http://www.buffalo.edu/"><img border="0" width="40" src="../images/SUNY-Buffalo.png"/></a></span>
                        <span><a href="https://www.zju.edu.cn/english/"><img border="0" width="40" src="../images/zju.png"/></a></span>
                        <span><a href="https://www.nd.edu/"><img border="0" width="40" src="../images/nd3.png"/></a></span>
                    </div>

                    <!-- Collect the nav links, forms, and other content for toggling -->
                    <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                        <ul class="nav navbar-nav navbar-right">
                            <li class="dropdown">
                                <a class="dropdown-toggle" data-toggle="dropdown" href="#">
                                    Home<span class="caret"></span></a>
                                <ul class="dropdown-menu">
                                    <li><a href="../index.html#intro">Introduction</a></li>
                                    <!-- <li><a href="bio.html">Biography</a></li> -->
                                    <li><a href="../cv.html">CV (Web)</a></li>
                                    <li><a target="_blank" href="../cv_xiaoweixu.pdf">CV (PDF)</a></li>
                                </ul>
                            </li>
                            <li class="page-scroll">
                                <a href="../index.html#news">News</a>
                            </li>
                            <li class="dropdown">
                                <a class="dropdown-toggle" data-toggle="dropdown" href="#">
                                    Publications<span class="caret"></span></a>
                                <ul class="dropdown-menu">
                                    <li><a onclick="javascript:reset_menus();$('#tab-3-content').show();" href="../index.html#publications">Selected</a></li>
                                    <li><a href="../publications/">All</a></li>
                                </ul>
                            </li>
                            <!-- <li class="page-scroll">
                                <a href="talks/">Talks</a>
                            </li>
                            <li class="page-scroll">
                                <a href="index.html#courses">Courses</a>
                            </li>
                            <li class="page-scroll">
                                <a href="index.html#awards">Awards</a>
                            </li> -->
                            <li class="page-scroll">
                                <a href="../index.html#service">Service</a>
                            </li>
                            <li class="page-scroll">
                                <a href="../index.html#resources">Resources</a>
                            </li>
                            <!-- <li class="page-scroll">
                                <a target="_blank" href="http://dmsl.cs.ucy.ac.cy/projects.php">Grants</a>
                            </li> -->

                            <li class="page-scroll">
                                <a href="../index.html#contact">Contact</a>
                            </li>
                            <li class="page-scroll">
                                <a onclick="$('#bs-example-navbar-collapse-2').toggle();">
                                    <span class="glyphicon glyphicon-search"></span>
                                </a>
                            </li>
                        </ul>
                    </div><!-- /.navbar-collapse -->

                    <!-- search box submenu -->
                    <div class="collapse" id="bs-example-navbar-collapse-2">
                        <gcse:search></gcse:search>
                    </div>

                </div><!-- /.container-fluid -->
            </nav>

            <section>
                <!-- Place this tag where you want the search results to render -->
                <gcse:searchresults-only></gcse:searchresults-only>
            </section>

            <section id="tree" style="margin-top:50px">
                <div class="container">
                    <a href="../index.html">Xiaowei Xu</a> > Recordings
                </div>
            </section>


            <!-- Home Section -->
            <section id="home">
                <div class="container lead">

                    <h3> Dec, 2020 </h3>

                    <li><span class="label label-success">1 Dec, 2020</span><a href="https://arxiv.org/pdf/2003.07923.pdf">"" </a> </li>
                        <p> <span class="label label-default">extensive</span> 
                        </p>While many studies were encountered utilizing transfer learning with unlabeled medical data for classification and diagnosis tasks, only a few cases were found adopting this approach for segmentation. This work investigates two strategies for combining knowledge from labeled and unlabeled data for segmentation.
                        </p>The first approach uses a WTA-CAE to learn features from unlabeled images and uses those features to initialize the weights of an F-Net that is then trained to perform the segmentation task. This strategy can be likened to pretraining of the segmentation network with self-supervision and is hereafter denoted by S-TL.
                        </p>The second approach simultaneously trains an F-Net for segmentation and WTA-CAE for reconstruction,which corresponds to multi-task learning with a self-supervised task, is denoted by S-MTL.
                        </p>Results show both training strategies are more effective with a large ratio of unlabeled to labeled training data. Multi-task learning performed slightly better, but tends to demand more computational resources than the transfer learning strategy.

                        </p>In accordance with the original paper [4], the larger kernel produced better results for the liver segmentation task.
                        </p><b>Larger patches resulted in higher DSC</b>.
                        </p><b>A combined Dice - Cross Entropy (Dice-CE) loss is more favorable than Focal loss</b>.
                        </p>The original paper increased the number of feature maps for coarser resolution levels, with [16, 24, 32, 48] for resolutions 0 − 3. Here, the number of feature maps per level was kept constant, [32, 32, 32, 32], to allow a uniform sparsity level for the autoencoders.  Using a uniform number of feature maps slightly lowered the average DSC, but also slightly lessened the standard deviation. The drop in performance was regarded as negligible and all further experiments used a uniform number of feature maps for the sake of the other training strategies.
                        </p>Spatial sparsity is achieved in the network by retaining only the highest activations per feature map in the bottleneck, creating a sparsity level that is proportional to the number of feature maps. Each minibatch activates different filters in the feature maps, which circumvents the dead filter problem. Sparsity is an essential form of regularization, since without it the network would learn useless delta functions which merely copy the input instead of extracting useful features.
                        </p>Generally, the earlier layers of a CNN learn common, low level image features, whereas the later layers learn detailed, task specific features. For most applications of transfer learning, fine-tuning only the later pretrained layers is often considered sufficient for computer vision tasks. However, as shown by [37] layer-wise fine-tuning experiments are necessary in order to determine the optimal transfer procedure for a specific application.
                        <p> <span class="label label-default">comment</span>  
                    <li><span class="label label-success">2 Dec, 2020</span><a href="https://arxiv.org/pdf/1805.11247.pdf">"MICROSCOPY CELL SEGMENTATION VIA CONVOLUTIONAL LSTM NETWORKS
                        " </a> </li>
                        <p> <span class="label label-default">extensive</span> 
                        </p>In order to exploit cell dynamics we propose a novel segmentation architecture which integrates Con- volutional Long Short Term Memory (C-LSTM) with the U-Net.
                        </p><b>For unet, adding C-LSTM to the encoder obtains higher accuracy than to the decoder</b>.

                        </p> U-net and FCNs are limited by their inability to incorporate temporal information, which can facilitate segmenta- tion of individual touching cells or of cells that are par- tially visible.
                        </p>The C-LSTM has re- cently been used to address the analysis of both temporal image sequences, such as next frame prediction [16], and volu- metric data sets [17, 18]. In [18] C-LSTM is applied in multi- ple directions for the segmentation of 3D data represented as a stack of 2D slices. Another approach for 3D brain structure segmentation is proposed in [17], where each slice is separately fed into a U-Net architecture, and only the output then fed into bi-directional C-LSTMs.
                        </p>We note that, unlike [17] which was designed and evaluated on 3D brain segmentation, the proposed novel architec- ture is an intertwined composition of the two concepts rather than a pipeline.
                        <p> <span class="label label-default">comment</span>  The improvement is relatively small. Dataset and code are published as
                                https: //github.com/arbellea/LSTM-UNet.git.

                    <li><span class="label label-success">3 Dec, 2020</span><a href="https://arxiv.org/pdf/1802.06955.pdf">"Recurrent Residual Convolutional Neural Network based on U-Net (R2U-Net) for Medical Image Segmentation
                        " </a> </li>
                        <p> <span class="label label-default">extensive</span> 
                            </p>We propose a Recurrent Convolutional Neural Network (RCNN) based on U-Net as well as a Recurrent Residual Convolutional Neural Network (RRCNN) based on U-Net models, which are named RU-Net and R2U-Net respectively. The proposed models utilize the power of U-Net, Residual Network, as well as RCNN

                            </p>There are some limitations of medical image segmentation including data scarcity and class imbalance.
                            </p>Different data transformation or augmentation techniques (data whitening, rotation, translation, and scaling) are applied for increasing the number of labeled samples available [12, 13, and 14].
                            </p>In addition, <b>patch based approaches are used for solving class imbalance problems</b>.
                            </p>The architecture for segmentation tasks generally requires almost double the number of network parameters when compared to the architecture of the classification tasks.
                            </p>The U-Net model provides several advantages for segmentation tasks: first, this model allows for the use of global location and context at the same time. Second, it works with very few training samples and provides better performance for segmentation tasks [12]. Third, an end-to-end pipeline process the entire image in the forward pass and directly produces segmentation maps. This ensures that U-Net preserves the full context of the input images, which is a major advantage when compared to patch-based segmentation approaches [12, 14].
                        <p> <span class="label label-default">comment</span>  
                    
                    <li><span class="label label-success">4 Dec, 2020</span><a href="https://arxiv.org/pdf/1608.04117.pdf">"The Importance of Skip Connections in Biomedical Image Segmentation" </a> </li>
                        <p> <span class="label label-default">extensive</span> 
                            </p><b>The recent results suggest that depth can act as a regularizer</b>.
                            </p>cshort skip connections appear to stabilize updates</b>.
                            </p>The variant with both long and short skip connections is not only the one that performs best but also converges faster than without short skip connections.
                            </p>Long skip connections are retained, at least the shallow parts of the model can be updated (see both sides of Figure 4(b)) as these connections provide shortcuts for gradient flow
                            </p>Batch normalization was observed to increase the maximal updatable depth of the network
                            </p><b>Even randomly initialized weights can confer a surprisingly large portion of a model’s performance after training only the classifier</b>.
                            </p><b>Although long skip connections provide a shortcut for gradient flow in shallow layers, they do not alleviate the vanishing gradient problem in deep networks</b>.
 
                        <p> <span class="label label-default">comment</span>  

                    <li><span class="label label-success">5 Dec, 2020</span><a href="https://arxiv.org/pdf/1604.02677.pdf">"DCAN: Deep Contour-Aware Networks for Accurate Gland Segmentation" </a> </li>
                        <p> <span class="label label-default">extensive</span> 
                            </p>Based on the FCN, we push it further by harnessing multi-level contextual feature representations, which include different levels of contextual information.
                            </p>Direct training a network with such a large depth may fall into a local minima. So weighted auxiliary classifiers C1-C3 are added into the network to further strengthen the training process.
                            </p>However, it’s still quite hard to separate the touching glands by leveraging only on the likelihood of gland objects due to the essential ambiguity in touching regions. This is rooted in the downsampling path causing spatial information loss along with feature abstraction. The boundary information formed by epithelial cell nuclei provides good complementary cues for splitting objects.

                            </p>When incorporated with multi-task regularization during the training, the discriminative capability of intermediate features can be further improved
                            </p><b>The motivation behind this is that the downsampling path aims at extracting the high level abstraction information, while the upsampling path predicting the score masks in a pixel-wise way.</b>
                            
                        <p> <span class="label label-default">comment</span>  

                    <li><span class="label label-success">6 Dec, 2020</span><a href="https://arxiv.org/pdf/1809.10486.pdf">"nnU-Net: Self-adapting Framework for U-Net-Based Medical Image Segmentation" </a> </li>
                        <p> <span class="label label-default">extensive</span> 
                            </p><b>We argue the strong case for taking away superfluous bells and whistles of many proposed network designs and instead focus on the remaining aspects that make out the performance and generalizability of a method.</b>
                            </p>The Medical Segmentation Decathlon is intended to specifically address this issue: participants in this challenge are asked to create a segmentation algorithm that generalizes across 10 datasets corresponding to different entities.
                            </p>These algorithms may dynamically adapt to the specifics of a particular dataset, but are only allowed to do so in a fully automatic manner.
                            </p><b>We hypothesize that some of the architectural modifications presented recently are in part overfitted to specific problems or could suffer from imperfect validation that results from sub-optimal reimplementations of the state-of-the-art.</b>
                            </p><b>We believe that the remaining interdependent choices regarding the exact architecture, pre- processing, training, inference and post-processing quite often cause the U-Net to underperform when used as a benchmark.</b>
                            </p>These are steps where much of the nets’ performance can be gained or respectively lost: preprocessing (e.g. resampling and normalization), training (e.g. loss, optimizer setting and data augmentation), inference (e.g. patch-based strategy and ensembling across test-time augmentations and models) and a potential post-processing (e.g. enforcing single connected components if applicable).
                            </p>All intensity values occurring within the segmentation masks of the training dataset are collected and the entire dataset is normalized by clipping to the [0.5, 99.5] percentiles of these intensity values, followed by a z- score normalization based on the mean and standard deviation of all collected intensity values.
                            </p>A connected component analysis of all ground truth segmentation labels is performed on the training data.

                            </p> 
                        <p> <span class="label label-default">comment</span>  

                    <li><span class="label label-success">7 Dec, 2020</span><a href="https://arxiv.org/pdf/1606.04797.pdf">"V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation
                        " </a> </li>
                        <p> <span class="label label-default">extensive</span> 
                            </p><b>Motivated by [16] and other works discouraging the use of max-pooling operations in CNNs, pooling layers have been replaced in our approach by convolutional ones.</b>
                            </p>Replacing pooling operations with convolutional ones results also to networks that, depending on the specific implementation, can have a smaller memory footprint during training, due to the fact that no switches mapping the output of pooling layers back to their inputs are needed for back-propagation, and that can be better understood and analysed [19] by applying only de-convolutions instead of un-pooling operations.
                            
                            </p>We introduce a novel objective function, that we optimise during training, based on Dice coefficient. In this way we can deal with situations where there is a strong imbalance between the number of foreground and background voxels.
                            </p><b>Using Dice loss,  we do not need to assign weights to samples of different classes to establish the right balance between foreground and background voxels.</b>
                        <p> <span class="label label-default">comment</span>  

                            


                        </div>
                </div>
            </section>

            <hr class="star-primary">
            <footer>
                <small>
                    <center>
                        © 2016 | D. Zeinalipour. Credits: AR template
                        <a onclick="javascript:$('#credit').toggle();"><img border="0" src="images/ccby.png"/></a>
                        <div style="display:none;" id="credit">[AR template available under Creative Commons CC BY 4.0 licence:
                            <a href="https://github.com/dmsl/academic-responsive-template" target="_blank">
                                https://github.com/dmsl/academic-responsive-template
                            </a> ]
                        </div>
                    </center>
                </small>
            </footer>
            
        </body>
        
</html>
