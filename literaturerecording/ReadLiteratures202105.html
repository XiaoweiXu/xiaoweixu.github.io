<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8"/>
        <title>[ Xiaowei Xu | News ]</title>
        <meta name="viewport" content="width=device-width, initial-scale=1.0"/>

        <!-- Font Awesome Icons -->
        <link rel="stylesheet" href="../css/font-awesome.min.css"/>

        <!-- Bootstrap -->
        <link href="../css/bootstrap.min.css" rel="stylesheet"/>
        <!--<link href="css/bootstrap.min.css" rel="stylesheet">-->

        <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
        <!--[if lt IE 9]>
         <script src="js/html5shiv.js"></script>
         <script src="js/respond.min.js"></script>
         <![endif]-->


        <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
        <script src="../js/jquery.min.js"></script>
        <!-- Include all compiled plugins (below), or include individual files as needed -->
        <script src="../js/bootstrap.min.js"></script>
        <script src="../js/menucollapse.js"></script>
        <script type="text/javascript" src="js/arrow78.js"></script>
        <script type="text/javascript" src="js/custom.js"></script>

        <body id="page-top" class="index">

            <!-- Navigation -->
            <nav class="navbar navbar-default navbar-fixed-top">
                <div class="container-fluid">
                    <!-- Brand and toggle get grouped for better mobile display -->
                    <div class="navbar-header">
                        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"  data-target="#bs-example-navbar-collapse-2" aria-expanded="false">
                            <span class="sr-only">Toggle navigation</span>
                            <span class="glyphicon glyphicon-search"></span>
                        </button>
                        <button id="button2" type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
                            <span class="sr-only">Toggle navigation</span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                        </button>
                        <span><a href="http://www.gdghospital.org.cn/ziwang/index.php/"><img border="0" width="200" src="../images/gdsxxgbyjs_xxw.png"/></a></span>
                        <span><a href="http://english.hust.edu.cn/"><img border="0" width="40" src="../images/Hustseals.png"/></a></span>
                        <span><a href="https://www.ualberta.ca/index.html"><img border="0" width="40" src="../images/ua.png"/></a></span>
                        <span><a href="http://www.buffalo.edu/"><img border="0" width="40" src="../images/SUNY-Buffalo.png"/></a></span>
                        <span><a href="https://www.zju.edu.cn/english/"><img border="0" width="40" src="../images/zju.png"/></a></span>
                        <span><a href="https://www.nd.edu/"><img border="0" width="40" src="../images/nd3.png"/></a></span>
                    </div>

                    <!-- Collect the nav links, forms, and other content for toggling -->
                    <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                        <ul class="nav navbar-nav navbar-right">
                            <li class="dropdown">
                                <a class="dropdown-toggle" data-toggle="dropdown" href="#">
                                    Home<span class="caret"></span></a>
                                <ul class="dropdown-menu">
                                    <li><a href="../index.html#intro">Introduction</a></li>
                                    <!-- <li><a href="bio.html">Biography</a></li> -->
                                    <li><a href="../cv.html">CV (Web)</a></li>
                                    <li><a target="_blank" href="../cv_xiaoweixu.pdf">CV (PDF)</a></li>
                                </ul>
                            </li>
                            <li class="page-scroll">
                                <a href="../index.html#news">News</a>
                            </li>
                            <li class="dropdown">
                                <a class="dropdown-toggle" data-toggle="dropdown" href="#">
                                    Publications<span class="caret"></span></a>
                                <ul class="dropdown-menu">
                                    <li><a onclick="javascript:reset_menus();$('#tab-3-content').show();" href="../index.html#publications">Selected</a></li>
                                    <li><a href="../publications/">All</a></li>
                                </ul>
                            </li>
                            <!-- <li class="page-scroll">
                                <a href="talks/">Talks</a>
                            </li>
                            <li class="page-scroll">
                                <a href="index.html#courses">Courses</a>
                            </li>
                            <li class="page-scroll">
                                <a href="index.html#awards">Awards</a>
                            </li> -->
                            <li class="page-scroll">
                                <a href="../index.html#service">Service</a>
                            </li>
                            <li class="page-scroll">
                                <a href="../index.html#resources">Resources</a>
                            </li>
                            <!-- <li class="page-scroll">
                                <a target="_blank" href="http://dmsl.cs.ucy.ac.cy/projects.php">Grants</a>
                            </li> -->

                            <li class="page-scroll">
                                <a href="../index.html#contact">Contact</a>
                            </li>
                            <li class="page-scroll">
                                <a onclick="$('#bs-example-navbar-collapse-2').toggle();">
                                    <span class="glyphicon glyphicon-search"></span>
                                </a>
                            </li>
                        </ul>
                    </div><!-- /.navbar-collapse -->

                    <!-- search box submenu -->
                    <div class="collapse" id="bs-example-navbar-collapse-2">
                        <gcse:search></gcse:search>
                    </div>

                </div><!-- /.container-fluid -->
            </nav>

            <section>
                <!-- Place this tag where you want the search results to render -->
                <gcse:searchresults-only></gcse:searchresults-only>
            </section>

            <section id="tree" style="margin-top:50px">
                <div class="container">
                    <a href="../index.html">Xiaowei Xu</a> > Recordings
                </div>
            </section>


            <!-- Home Section -->
            <section id="home">
                <div class="container lead">

                    <h3> May, 2021 </h3>
                     
                    <li><span class="label label-success">7 May, 2020</span><a href="https://arxiv.org/abs/2010.11929">
                        " An Image is Worth 16×16 Words: Transformers for Image Recognition at Scale" </a> </li>
                            <p> <span class="label label-default">extensive</span> 
                            <p><b>Original Abstract</b></p>
                            <p>While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer can perform very well on image classification tasks when applied directly to sequences of image patches. When pre-trained on large amounts of data and transferred to multiple recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer attain excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.</p>
                            <p><b>Our Summary </b></p>
                            <p>The authors of this paper submitted anonymously to ICLR 2021 show that a pure Transformer can perform very well on image classification tasks. They introduce Vision Transformer (ViT), which is applied directly to sequences of image patches by analogy with tokens (words) in NLP. When trained on large datasets of 14M–300M images, Vision Transformer approaches or beats state-of-the-art CNN-based models on image recognition tasks. In particular, it achieves an accuracy of 88.36% on ImageNet, 90.77% on ImageNet-ReaL, 94.55% on CIFAR-100, and 77.16% on the VTAB suite of 19 tasks.
                            <p><b>What’s the core idea of this paper?</b></p>
                            <p>When applying Transformer architecture to images, the authors follow as closely as possible the design of the original Transformer designed for NLP.
                                The introduced Transformer-based approach to image classification includes the following steps:
                                splitting images into fixed-size patches;
                                linearly embedding each of them;
                                adding position embeddings to the resulting sequence of vectors;
                                feeding the patches to a standard Transformer encoder;
                                adding an extra learnable ‘classification token’ to the sequence.
                                Similarly to Transformers in NLP, Vision Transformer is typically pre-trained on large datasets and fine-tuned to downstream tasks.</p>
                            <p><b>What’s the key achievement?</b></p>
                            <p>Vision Transformer pre-trained on the JFT300M dataset matches or outperforms ResNet-based baselines while requiring substantially less computational resources to pre-train. It achieves an accuracy of:
                                88.36% on ImageNet; 
                                90.77% on ImageNet-ReaL; 
                                94.55% on CIFAR-100; 
                                97.56% on Oxford-IIIT Pets;
                                99.74% on Oxford Flowers-102;
                                77.16% on the VTAB suite of 19 tasks.</p>
                            <p><b>What does the AI community think?</b></p>
                            <p>The paper is trending in the AI research community, as evident from the repository stats on GitHub.
                                It has also been accepted for oral presentation at ICLR 2021, one of the key conferences in deep learning.
                                </p>
                            <p><b>What are future research areas?</b></p>
                            <p>Applying Vision Transformer to other computer vision tasks, such as detection and <b>segmentation</b>.
                                Exploring self-supervised pre-training methods.
                                Analyzing the few-shot properties of Vision Transformer.
                                Exploring contrastive pre-training.
                                Further scaling ViT.</p>
                            <p><b>Where can you get implementation code?</b></p>
                            <p>The PyTorch implementation of Vision Transformer is available on GitHub.</p>    

                    <li><span class="label label-success">7 May, 2020</span><a href="https://arxiv.org/pdf/2009.14794.pdf">
                        "Rethinking Attention with Performers" </a> </li>
                            <p> <span class="label label-default">extensive</span> 
                            <p><b>Original Abstract</b></p>
                            <p>We introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, 
                                but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. 
                                To approximate softmax attention-kernels, Performers use a novel Fast Attention Via positive Orthogonal Random features approach (FAVOR+), 
                                which may be of independent interest for scalable kernel methods. FAVOR+ can be also used to efficiently model kernelizable attention mechanisms 
                                beyond softmax. This representational power is crucial to accurately compare softmax with other kernels for the first time on large-scale tasks, 
                                beyond the reach of regular Transformers, and investigate optimal attention-kernels. Performers are linear architectures fully compatible with regular
                                 Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low estimation
                                  variance. We tested Performers on a rich set of tasks stretching from pixel-prediction through text models to protein sequence modeling. We demonstrate
                                   competitive results with other examined efficient sparse and dense attention methods, showcasing effectiveness of the novel attention-learning paradigm
                                    leveraged by Performers.</p>
                            <p><b>Our Summary </b></p>
                            <p>The authors from Google and DeepMind propose an efficient transformer architecture called Performer. The attention module in the standard transformer architecture has quadratic space and time complexity, which makes it inefficient to scale to long-sequence inputs. Most of the existing techniques for efficient attention modules rely on a sparsity assumption, which has to be verified empirically by trial and error. The remaining techniques are less applicable to tasks with long-sequence inputs because of their poor performance processing long sequences. Performer, on the other hand, doesn’t rely on any assumptions such as sparsity or low-rankness and is provably accurate in approximating the softmax attention values. Performer uses a scalable kernel method termed Fast Attention Via positive Orthogonal Random features approach (FAVOR+). This method can be applied to efficiently model other kernelizable attention mechanisms beyond softmax and presents a framework to compare softmax alternatives. Performer shows competitive results compared to other efficient sparse and dense attention methods on a rich set of tasks ranging from pixel-prediction to text models to protein sequence modeling.</p>
                            <p><b>What’s the core idea of this paper?</b></p>
                            <p>The authors proposed a scalable kernel method, FAVOR+, that:
                                approximates the standard attention weights without any assumptions about sparsity and low-rankness;
                                provides strong theoretical guarantees such as unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence, and lower variance of the approximation;
                                enables softmax to be accurately compared to other kernelizable attention mechanisms that are beyond the standard transformer architecture;
                                can be combined with ideas for efficient transformers like reversible layers or cluster-based attention.</p>
                            <p><b>What’s the key achievement?</b></p>
                            <p>It is shown empirically that the Performer can be 2× faster than Reformer, the best of the current efficient transformer architectures.
                            </p>
                            <p><b>What does the AI community think?</b></p>
                            <p>The paper has been accepted for oral presentation at ICLR 2021, one of the key conferences in deep learning.
                            </p>
                            <p><b>What are future research areas?</b></p>
                            <p>Exploring more optimal attention mechanisms with the help of the proposed FAVOR+ framework.
                            </p>
                            <p><b>Where can you get implementation code?</b></p>
                            <p>The implementation of Performer is available on GitHub.
                            </p>

                        <li><span class="label label-success">7 May, 2020</span><a href="https://arxiv.org/pdf/2008.02217.pdf">
                            "Hopfield Networks is All You Need," </a> </li>
                                <p> <span class="label label-default">extensive</span> 
                                <p><b>Original Abstract</b></p>
                                <p>We introduce a modern Hopfield network with continuous states and a corresponding update rule. The new Hopfield network can store exponentially (with the dimension of the associative space) many patterns, retrieves the pattern with one update, and has exponentially small retrieval errors. It has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. The new update rule is equivalent to the attention mechanism used in transformers. This equivalence enables a characterization of the heads of transformer models. These heads perform in the first layers preferably global averaging and in higher layers partial averaging via metastable states. The new modern Hopfield network can be integrated into deep learning architectures as layers to allow the storage of and access to raw input data, intermediate results, or learned prototypes. These Hopfield layers enable new ways of deep learning, beyond fully-connected, convolutional, or recurrent networks, and provide pooling, memory, association, and attention mechanisms. We demonstrate the broad applicability of the Hopfield layers across various domains. Hopfield layers improved state-of-the-art on three out of four considered multiple instance learning problems as well as on immune repertoire classification with several hundreds of thousands of instances. On the UCI benchmark collections of small classification tasks, where deep learning methods typically struggle, Hopfield layers yielded a new state-of-the-art when compared to different machine learning methods. Finally, Hopfield layers achieved state-of-the-art on two drug design datasets. The implementation is available at https://github.com/ml-jku/hopfield-layers.
                                </p>
                                <p><b>Our Summary </b></p>
                                <p>The authors present a modern Hopfield network with continuous states and an update rule equivalent to the attention mechanism in Transformers. In the context of this work, a Hopfield network represents an energy function that gives the energy of every possible data point (input and output) and an update rule which changes that energy function based on observed data so that the network converges to an energy minimum. With continuous states and an update rule equivalent to attention modules, the authors claim that the proposed Hopfield network would be a general framework that could be used as a pooling layer, GRU or LSTM layer, and attention layer.
                                <p> The proposed Hopfield networks are continuous and differentiable with respect to their parameters and can be incorporated into any deep learning architecture as a component. As a general framework, the proposed Hopfield networks provide pooling, memory, association, and attention mechanisms, which enables them to summarize a set of vectors, perform better at multiple instance learning (MIL), and use associative memories, among other capabilities. The results achieved by the proposed methods included superior performance on multiple MIL datasets and better performance than standard ML methods on UCI datasets.
                                <p><b>What’s the core idea of this paper?</b></p>
                                <p>The authors introduce different types of Hopfield layers for different tasks:
                                    Layer Hopfield: This layer takes two sets of vectors and processes the association between those two sets. This layer could replace the attention module in the standard transformer architecture. With this capability, this layer could be used for sequence-to-sequence learning or any operations on point sets (sets of vectors).
                                    Layer HopfieldPooling: This layer takes a set of vectors and produces a summarization of that set of vectors. This layer has a list of queries where each query produces a vector as an output. Here, the query is the key and the input set of vectors are values in the attention module. So, the output of this layer could be a single vector or a set of vectors based on the number of queries in the layer. This type of layer could be used for multiple-instance learning, as it can produce a summary of n instances.
                                    Layer HopfieldLayer: This layer takes a set of vectors and produces a set of vectors. The layer has another set of vectors in memory, which could be a fixed set or trainable vectors. The output set of vectors is the output of the attention module, where the vectors in memory act as keys and the input set of vectors act as values. This generic layer could approximate Support Vector Machines (SVMs), k-nearest neighbor, approaches that learn vector quantization, and methods for pattern search.</p>
                                <p><b>What’s the key achievement?</b></p>
                                <p>The proposed Hopfield networks:
                                    showed superior performance on multiple-instance learning datasets such as Tiger, Fox, Elephant, and UCSB;
                                    outperformed standard machine learning methods and deep learning methods on UCI Benchmark Collection tabular datasets.
                                </p>
                                <p><b>What does the AI community think?</b></p>
                                <p>Studying the advantages and limitations of GRUs/LSTMs, transformers, memory-based neural networks, and pooling layers compared to their Hopfield network equivalentss.
                                </p>
                                <p><b>What are future research areas?</b></p>
                                <p>The proposed Hopfield network could be instantiated into many different models where the applications include information retrieval, sequence classification, detecting outliers or border cases, and drug design, among other tasks.                                    .
                                </p>
                                <p><b>Where can you get implementation code?</b></p>
                                <p>The implementation of the proposed Hopfield Networks is available on GitHub.
                                </p>
        

                            </div>
            </section>

            <hr class="star-primary">
            <footer>
                <small>
                    <center>
                        © 2016 | D. Zeinalipour. Credits: AR template
                        <a onclick="javascript:$('#credit').toggle();"><img border="0" src="images/ccby.png"/></a>
                        <div style="display:none;" id="credit">[AR template available under Creative Commons CC BY 4.0 licence:
                            <a href="https://github.com/dmsl/academic-responsive-template" target="_blank">
                                https://github.com/dmsl/academic-responsive-template
                            </a> ]
                        </div>
                    </center>
                </small>
            </footer>
            
        </body>
        
</html>
